<script type="text/javascript" language="JavaScript" src='/js/header.js'></script>
<!-- Start content - do not edit above this line  -->
<script type='text/javascript' language='JavaScript'>document.querySelector('title').textContent = 'UNet: a convolutional network for biomedical image segmentation ';</script>
<div class="title"><b>UNet: a convolutional network for biomedical image segmentation </b></div>

<table width=25% align=right>
  <tr>
    <td>
      <div class="toc">
        <div class="tocHeading">Quick Links</div>
        <div class="tocItem"><A href="#doc">Documentation</a></div>
        <div class="tocItem"><a href="#notes">Notes</a></div>
        <div class="tocItem"><a href="#int">Interactive job </a></div>
        <div class="tocItem"><a href="#sbatch">Batch job </a></div>
        <div class="tocItem"><a href="#swarm">Swarm of jobs </a></div>
      </div>
</table>

<p>
<mark>UNet is a winner of the ISBI bioimage segmentation challenge 2015.</mark>
It relies on data augmentation to use the available annotated samples more efficiently. 
The architecture consists of a contracting path to capture context 
and a symmetric expanding path that enables precise localization. <br /> <br />
This application is being used as a biological example in class #1 of the course "Deep Learning by Example on Biowulf".
</p>

<h3>References:</h3>
<ul>
<li>Olaf Ronneberger, Philipp Fischer, Thomas Brox <br />
<i> U-Net: Convolutional Networks for Biomedical Image Segmentation </i><br />
<a href="https://arxiv.org/abs/1505.04597">arXiv:1505.04597 </a> 18 May, 2015 <br/>
<li>U-Net implementationi in Keras: <a href="https://github.com/zhixuhao/unet">https://github.com/zhixuhao/unet</a>
<li>Vincent Casser, Kai Kang, Hanspeter Pfister and Daniel Haehn <br />
<i> Fast Mitochondria Segmentation for Connectomics</i><br />
<a href="https://arxiv.org/abs/1812.06024">arXiv:1812.06024</a>
</ul>

<div class="heading">Important Notes</div>
<ul>
<li>Module Name: <tt>UNet</tt> (see <a href="https://hpc.nih.gov/apps/modules.html">the modules page</a> for more information)
<li>Unusual environment variables set 
  <ul>
    <li><b>UNET_HOME</b>&nbsp; UNet installation directory</li>
    <li><b>UNET_BIN </b>&nbsp; &nbsp; &nbsp; UNet executable directory</li>
    <li><b>UNET_SRC </b>&nbsp; &nbsp; a folder containing the source code</li>
    <li><b>UNET_DATA</b>&nbsp; &nbsp; a folder containing sample data</li>
  </ul>
</ul>
<P>

<a Name="int"></a><div class="heading">Interactive job</div>
<div class="nudgeblock"><a href="/docs/userguide.html#int">Interactive jobs</a> should be used for debugging, graphics, or applications that cannot be run as batch jobs.</div>
<p>Allocate an <a href="/docs/userguide.html#int">interactive session</a> and run the program. Sample session:</p>
<pre class="term">
[user@biowulf]$ <b>sinteractive --mem=16g --gres=gpu:v100:1,lscratch:10 -c4 </b>
[user@cn4471 ~]$<b>module load unet </b>
[+] Loading cuDNN/8.0.3/CUDA-11.0 libraries...
[+] Loading CUDA Toolkit  11.0.3  ...
[+] Loading gcc 4.8.5  ...
[+] Loading Qt 5.9.4  ...
[+] Loading UNet 20210223  ...
</pre>
The bin directory of the application includes three executable files: 
train.py, predict.py and visualize.py. 
In order to display the usage message and available command line options for an executable, 
type its name followed by the option "-h". For example:
<pre class="term">
[user@cn4471 ~]$<b> train.py -h </b>
...
usage: train.py [-h] -d data_folder [-a augmentation_rate] [-b batch_size] [--beta tversky_beta] [-c object_class]
                [--cw class_weights] [--drop_rate drop_rate] [-e num_epochs] [-F start_filters] [-g num_gpus]
                [--gamma tversky_gamma] [-l learning_rate] [--loss loss_type] [-n] [-s] [--save_augmented] [-v]
                [-w] [-X X] [-Y Y]

optional arguments:
  -h, --help            show this help message and exit
  -a augmentation_rate, --augmentation_rate augmentation_rate
                        by how many folds to increase the effective data size; default=20
  -b batch_size, --bs batch_size
                        batch size; default=2
  --beta tversky_beta   class balancing weight in the Trersky index: TI = TP/(TP + beta*FP + (1-beta)*FN)
  -c object_class, --object_class object_class
                        Detected object class(es): memb | mito | multi; default = membr
  --cw class_weights    Comma-separated class weights
  --drop_rate drop_rate
                        drop rate; default=0.5
  -e num_epochs, --num_epochs num_epochs
                        number of epochs; default=160
  -F start_filters, --start_filters start_filters
                        num. filters used in the 1st convolution of the network model; default=64 if
                        object_class=membr; =8 if object_class=mito; and =48 if object_class=multi
  -g num_gpus, --num_gpus num_gpus
                        number of gpus to use; default=1
  --gamma tversky_gamma
                        a power in the Trersky focal loss
  -l learning_rate, --lr learning_rate
                        learning rate; default=1.e-4
  --loss loss_type      loss type: bce | cce | wcce | dice | jaccard | tversky; default: wcce if object_class ==
                        'multi' and bce otherwise
  -n, --no_augmentation
                        don't perform data augmentation, i.e. use original input data; default=False
  -s, --summary         print the model summary
  --save_augmented      save augmented data (in the subfolder 'augmented'
  -v, --verbose         increase the verbosity level of output
  -w, --load_weights    read weights from a checkpoint file
  -X X, --image_width X
                        image width; should be multiple of 16; default=256
  -Y Y, --image_height Y
                        image height; should be multiple of 16; default=256

required arguments:
  -d data_folder, --data_folder data_folder
                        data folder name, e.g. 'data_isbi' or 'data_hhmi'
</pre>
In order to run the training executable on available sample data, first compy the data to your current folder:
<pre class="term">
[user@cn4471 ~]$<b> cp -r $UNET_DATA/* .</b>
</pre>
There are currently two sample datasets available, 
both comprising 2D EM images of Drosophila brain slices. 
The first dataset includes 30 pre-processed grayscale images 
together with corresponding binary masks for neural membranes
from the <a href="http://brainiac2.mit.edu/isbi_challenge/">ISBI Challenge</a>. 
It comes together with the Keras <a href="https://github.com/zhixuhao/unet">UNet implementation</a> code available at GitHub. This dataset is stored in the folder "data_isbi". <br /> <br />
The second dataset, stored in the folder data_hhmi, includes 24 pre-processed grayscale images 
together with the corresponrding binary masks for both the neural membranes and mitochondria.
This more challenging dataset was generated as a part of the 
<a href="https://www.sciencedirect.com/science/article/pii/S0092867418307876">Fly Brain Connectome project</a> 
conducted at the Howard Hughes Medical Institute.  <br /> <br />
Here is the command to train the UNet on the augmented data from the 1st dataset under default  options:
<pre class="term">
[user@cn4471 ~]$<b> train.py -d data_isbi</b>
Using Tensorflow backend.
...
Epoch 1/100
300/300 [==============================] - 28s 92ms/step - loss: 0.6890 - acc: 0.7793
Epoch 2/100
300/300 [==============================] - 21s 71ms/step - loss: 0.6809 - acc: 0.7817
Epoch 3/100
300/300 [==============================] - 21s 71ms/step - loss: 0.6731 - acc: 0.7815
Epoch 4/100
...
Epoch 99/100
300/300 [==============================] - 21s 71ms/step - loss: 0.0979 - acc: 0.9765
Epoch 100/100
300/300 [==============================] - 21s 71ms/step - loss: 0.0965 - acc: 0.9766
</pre>
The trainig results, i.e. model weights, will be stored in the checkpoint file stored in the folder "checkpoints", in the HDF5 format,
<pre class="term">
<b>checkpoints/&ltmodel_name&gt.&ltdata_folder_name&gt.&ltobject_class&gt.h5</b>
</pre>
in this particular case - in the file:
<pre class="term">
<b>checkpoints/unet.data_isbi.membr.h5</b>. 
</pre>
The prefix of the output checkpoint file can be changed through a command line option of the train.py. 
<br /> <br />
We can now use this file to predict membrane masks using as input 30 unaugmented grayscale images:
<pre class="term">
[user@cn4471 ~]$<b> predict.py -d data_isbi</b>
Using TensorFlow backend.
...
30/30 [==============================] - 2s 81ms/step
...
</pre>
For each the grayscale image file i.png (i=0,1,...,29), this command will produce a binary mask 
i_predict.png together with an RGB image i_predict_RGB.png with colored the connected components 
of the binary image. <br /> <br />
The predictions will be stored in the folder data_isbi/membrane/test.<br /> <br />
In order to visualize these results, use the visualize.py application:
<pre class="term">
[user@cn4471 ~]$<b> visualize.py -h </b>
usage: visualize.py [-h] [-c object_class] [-d data_folder] [-i image_path] [-n image_id]

optional arguments:
  -h, --help            show this help message and exit
  -c object_class, --object_class object_class
                        Detected object class(es): membr | mito | multi; default = membr
  -d data_folder, --data_folder data_folder
                        path to the top data folder
  -i image_path, --image image_path
                        a path to the image to be visualized
  -n image_id, --image_id image_id
                        a number in the range(num_images)
</pre>
Here, either the option -i or -n is required. For example, to visualize the 0-th data item, type: 
<pre class="term">
[user@cn4471 ~]$<b> visualize.py -n 0 -d data_isbi </b>
</pre>
<img src="unet/unet_membrane.png" width="800" border="0" alt="">
<br />
In order to use the second dataset, one can run similar train.py and predict.py commands, but with option "-d data_hhmi":
<pre class="term">
[user@cn4471 ~]$<b> train.py -d data_hhmi </b>
Using TensorFlow backend.
...
Epoch 1/100
300/300 [==============================] - 31s 104ms/step - loss: 0.1928 - acc: 0.9111
Epoch 2/100
300/300 [==============================] - 22s 72ms/step - loss: 0.1388 - acc: 0.9335
Epoch 3/100
300/300 [==============================] - 21s 71ms/step - loss: 0.1269 - acc: 0.9395
...
Epoch 159/160
240/240 [==============================] - 17s 72ms/step - loss: 0.0351 - acc: 0.9856
Epoch 160/160
240/240 [==============================] - 17s 72ms/step - loss: 0.0349 - acc: 0.9856
</pre>
This command will produce a checkpoint file hhmi.membrane.h5 in folder "checkpoints". <br />
Alternatively, a folder with already pre-computed checkpoint files can be copied from $UNET_DATA:
<pre class="term">
[user@cn4471 ~]$<b> cp -r $UNET_DATA/checkpoints . </b>
</pre>
Now we can run the executable predict.py:
<pre class="term">
[user@cn4471 ~]$<b> predict.py -d data_hhmi </b>
Using TensorFlow backend
...
24/24 [==============================] - 2s 98ms/step
...
[user@cn4471 ~]$<b> visualize.py -n 1 -d data_hhmi</b>
</pre>
<img src="unet/hhmi_membrane.png" width="800" border="0" alt="">
<br />
Likewise, training the unet model on the HHMI mitochondria data can be performed,
with subsequent prediction and visualization of the binary segmentation of mitochondria:
<pre class="term">
[user@cn4471 ~]$<b> train.py -d data_hhmi -c mito </b>
...
[user@cn4471 ~]$<b> predict.py -d data_hhmi -c mito </b>
...
[user@cn4471 ~]$<b> visualize.py -n 1 -d data_hhmi -c mito</b>
</pre>
<img src="unet/hhmi_mito.png" width="800" border="0" alt="">
<br />
<br />
Using additional command line options, one can also <br />
- store checkpoints for each epochs, rather than for the last epoch only; <br />
- output a summary of the network model; <br />
- change the data type for the 2nd dataset from "membrane" (default) to "mito" (=mitochondria); as well as<br />
- vary other hyper-parameters, such as the number of training epochs, the batch size, the number of images 
produced during augmentation of the training data, etc. <br /> <br />
In order to train the UNet using <b>multiple GPUs</b>,  <br />
- allocate a session with appropriate number of GPUs (you are allowed to use up to 4 GPUs per session), <br />
- specify through a command line option -g how many GPUs you want to use, and <br />
- specify a batch size that is multiple of the number of GPUs you will be using. <br />
For example:
<pre class="term">
[user@cn4471 ~]$ <b>exit</b>
[user@biowulf ~] <b>sinteractive --mem=16g --gres=gpu:v100:4,lscratch:40 -c14 </b>
[user@cn4471 ~]$ <b>module load unet </b>
[user@cn4471 ~]$ <b>cp -r $UNET_DATA/* .</b>
[user@cn4471 ~]$ <b>train.py -d data_isbi -g 4 -b 8 </b>
Using TensorFlow backend.
...
 StreamExecutor with strength 1 edge matrix:
2019-04-23 07:38:17.419226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3 
2019-04-23 07:38:17.419241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y N N 
2019-04-23 07:38:17.419252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N N N 
2019-04-23 07:38:17.419262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N Y 
2019-04-23 07:38:17.419271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   N N Y N
...
Epoch 1/160
18/18 [==============================] - 28s 2s/step - loss: 0.6806 - acc: 0.7497
Epoch 2/160
18/18 [==============================] - 11s 588ms/step - loss: 0.5285 - acc: 0.7811
Epoch 3/160
18/18 [==============================] - 11s 590ms/step - loss: 0.4687 - acc: 0.7814
...
</pre>
End the interactive session:
<pre class="term">
[user@cn4471 ~]$ <b>exit</b>
salloc.exe: Relinquishing job allocation 46116226
[user@biowulf ~]$
</pre>

<a Name="sbatch"></a><div class="heading">Batch job</div>
<div class="nudgeblock">Most jobs should be run as <A href="/docs/userguide.html#submit">batch jobs</a>.</div>
<p>Create a batch input file (e.g. unet.sh). For example:</p>
<pre class="term">
#!/bin/bash
module load unet 
cp -r $UNET_DATA/* .
train.py  -e 100 -l 0.0001
</pre>
<p>Submit this job using the Slurm <a href="/docs/userguide.html">sbatch</a> command.</p>

<pre class="term">sbatch [--cpus-per-task=#] [--mem=#] unet.sh</pre>

<!-- End content area - do not edit below this line -->
<script type="text/javascript" language="JavaScript" src='/js/footer.js'></script>
