<script type="text/javascript" language="JavaScript" src='/js/header.js'></script>
<!-- Start content - do not edit above this line  -->
<script type='text/javascript' language='JavaScript'>document.querySelector('title').textContent = ' CHARMM on Biowulf ';</script>
<div class="title">
CHARMM on Biowulf </div>
<P>
<table width=25% align=right> <tr><td>
<div class="toc">
<div class="tocHeading">Quick Links</div>
<div class="tocItem"><a href="#update">New CHARMM versions</a></div>
<div class="tocItem"><a href="#batch">Batch jobs</a></div>
<div class="tocItem"><a href="#gpus">Using GPUs</a></div>
<div class="tocItem"><a href="#interact">Interactive Usage</a></div>
<div class="tocItem"><a href="#param">Force Field Parameters</a></div>
<div class="tocItem"><A href="#resource">Other CHARMM resources</a></div>
</table>

<p>CHARMM (Chemistry at HARvard Macromolecular Mechanics) [1]:</p>
<ul>
<li>A thirty year history, with an emphasis on accuracy and precision,
under the leadership of Nobel prize winner Prof. Martin Karplus
<li>A versatile and widely used molecular simulation program with broad
application to many-particle systems</li>
<li>Has been developed with a primary focus on the study of molecules of
biological interest, including peptides, proteins, prosthetic groups, small
molecule ligands, nucleic acids, lipids, and carbohydrates, as they occur in
solution, crystals, and membrane environments</li>
<li>Provides a large suite of computational tools that encompass numerous
conformational and path sampling methods, free energy estimates, molecular
minimization, dynamics, analysis techniques, and model-building
capabilities</li>
<li>Can be utilized with various energy functions and models, from mixed
quantum mechanical-molecular mechanical force fields, to all-atom classical
potentials with explicit solvent and various boundary conditions, to implicit
solvent and membrane models</li></ul>

<center><b><i>Major new features in CHARMM 44 and 45:</i></b></center>
<ul>
  <li>New parameters for lignins added to the carbohydrate force field</li>
  <li>Numeous other force field improvements</li>
  <li>Support for DOMDEC_GPU on Volta-based GPUs</li>
  <li>DOMDEC may now be used with energy minimization (SD and ABNR) in some circumstances</li>
  <li>REPDSTR is now compatible with DOMDEC, greatly improving the performance of replica exchange simulations.</li>
  <li>New Lennard-Jones PME method (requires a seperate compile time option - contact staff if interested).</li>
  <li>Numerous minor enhancements and bug fixes.</li>
</ul>
<center>
[1] CHARMM: The Biomolecular simulation Program <i>J. Comp. Chem.</i> <b>30,</b> 1545-1615 (2009).<br>
[2] New faster CHARMM molecular dynamics engine <i>J. Comp. Chem.</i> <b>35,</b> 406-413 (2014).
</center>

<a class="navbar-safe-anchor"  Name="update"></a><div class="heading">New CHARMM versions!</div>

<p>As of October, 2020, CHARMM version c45b1 has been made available on Biowulf for general use. There
are <b>important changes</b> in how to run this version. In particular the &quot;cover scripts&quot;
that were previously used to run CHARMM have been retired. Instead, users must explicitly run
mpirun or srun for parallel runs and call the charmm program directly for serial runs.</p>

<p>For documentation on running CHARMM c42b2 and below with cover scripts, please see
<a href="/apps/charmm/older_charmm.html">the documentation for older releases</a>.</p> 

<p>The following modules are available:</p>

<p><ul>
  <li><tt>charmm/c45b1/serial</tt>: compiled with gcc/7.4.0 and X11 graphics - for use for running serial or interactive jobs; this version support graphics</li>
  <li><tt>charmm/c45b1/domdec</tt>: compiled with OpenMPI 4.0.5 and the Intel 2020.2.254 - for use with running parallel dynamics on <b>CPUs</b></li>
  <li><tt>charmm/c45b1/repdstr</tt>: compiled with OpenMPI 4.0.5 and the Intel 2020.2.254 - for use with running parallel dynamics <b>with replica exchange</b> on <b>CPUs</b></li>
  <li><tt>charmm/c45b1/gpu</tt>: compiled with OpenMPI 4.0.5, gcc/7.4.0, and CUDA/10.2 - for running DOMDEC_GPU on <b>GPUs</b>.</li>
</ul></p>

<a class="navbar-safe-anchor"  Name="batch"></a><div class="heading">SLURM Batch Jobs</div><p>
For a non-parallel CHARMM job such as model building or ad hoc trajectory analysis, the
commands and setup have few requirements.  The job script (build.csh) can be simply:<p>
<div class="term"><pre>
#!/bin/csh
cd $SLURM_SUBMIT_DIR
module load charmm/c45b1/serial

charmm < build-psf.inp >& build-psf.out
</pre></div><p>
The above can be submitted to the batch queue via:<p>
<div class="term"><pre>
sbatch build.csh
</pre></div><p>

For parallel usage, the following script (sbatch.sh) illustrates submitting a SLURM batch
job which will use the 28 physical cores on each of 4 nodes (112 total cores:<p>

<div class="term"><pre>
#!/bin/bash
#SBATCH --partition=multinode
#SBATCH --time=04:00:00
#SBATCH --ntasks=112
#SBATCH --nodes=4
#SBATCH --ntasks-per-core=1
#SBATCH --constraint="[x2680|x2695]"

module load charmm/c45b1/domdec

srun --mpi=pmix --ntasks=${SLURM_NTASKS} charmm -i input.inp -o output.out
</pre></div>

<p>The environment variable SLURM_SUBMIT_DIR points to the working directory where 'sbatch'
was run, and SLURM_NTASKS contains the value given with the <kbd>--ntasks=</kbd> argument
to sbatch. The above is suitable for most parallel CHARMM usage, other than the
DOMDEC_GPU code (see below).</p>

<p>Note that it is possible to specify <tt>sbatch</tt> parameters on the command line rather
than in the batch script itself. Command line parameters override those specified within the
batch script. E.g. <tt>sbatch --ntasks=56 --nodes=2 --ntasks-per-core=1 --partition=multinode sbatch.sh</tt>
would submit the job to 2 nodes via the command line.</p>

<a class="navbar-safe-anchor"  Name="gpus"></a><div class="heading">Using GPUs</div>

<p>The DOMDEC_GPU code may be used by using the <b>charmm/c45b1/gpu</b> module. DOMDEC_GPU
uses both an MPI library (OpenMPI in this case) and OpenMP threads, and therefore requires 
changes to the SLURM <b>sbatch</b> arguments.  The changes are shown in the example below (sbatchGPU.csh); 

<div class="term"><pre>
#!/bin/bash
#SBATCH --partition=gpu
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --gres=gpu:k80:1
#SBATCH --time=4:00:00

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

module load charmm/c45b1/gpu

srun --mpi=pmix --ntasks=$SLURM_NTASKS --cpus-per-task=$SLURM_CPUS_PER_TASK charmm -i input.inp -o output.out
</pre></div><p>

<p>This script runs DOMDEC_GPU on a single Biowulf K80 GPU and can be submitted with <tt>sbatch sbatchGPU.sh</tt>.
As with the CPU script, arguments can be given on the command line rather than in the script. Remember to modify
the wall time as needed!</p>

<a class="navbar-safe-anchor"  Name="interact"></a><div class="heading">Interactive Usage</div>

<p>For a variety of tasks such as model building, analysis, and graphics, foreground interactive use
of CHARMM can be advantageous, esp. when developing and testing a new input script.  The SLURM
<b>sinteractive</b> command makes this fairly easy (system prompts in <b>bold</b>, user input in <i>italics</i>):
<div class="term"><pre>
<b> biowulf /<2>EwaldNVE [69]</b> <i>sinteractive</i>
salloc.exe: Granted job allocation 1693180
salloc.exe: Waiting for resource configuration
salloc.exe: Nodes cn0032 are ready for job

<b> cn0032 /<2>EwaldNVE [1]</b> <i>module load charmm/c45b1/serial</i>
<b> cn0032 /<2>EwaldNVE [2]</b> <i>charmm &lt; build-psf.inp &gt;&amp; build-psf.out</i>
<b> cn0032 /<2>EwaldNVE [3]</b> <i>exit</i>
exit
salloc.exe: Relinquishing job allocation 1693180
salloc.exe: Job allocation 1693180 has been revoked.
</pre></div></p>

<p>
For troubleshooting, it may be useful to pipe the output, and both save it in file (via 'tee') and view it in the 'less' browser, e.g.:
<div class="term"><pre>
<b> cn0254 /<2>EwaldNVE [3]</b> charmm &lt; minmodel.inp |&amp; tee minmodel.out | less
</pre></div><p>
Finally, CHARMM itself can be run interactively, via simply:
<div class="term"><pre>
<b> biowulf /<2>EwaldNVE [71]</b> <i>sinteractive</i>
salloc.exe: Granted job allocation 1693592
salloc.exe: Waiting for resource configuration
salloc.exe: Nodes cn0103 are ready for job

<b> cn0103 /<2>EwaldNVE [1]</b> <i>module load charmm/c45b1/serial</i>
<b> cn0103 /<2>EwaldNVE [2]</b> <i>charmm</i>

                 Chemistry at HARvard Macromolecular Mechanics
           (CHARMM) - Developmental Version 45b1     August 15, 2020
                                Revision unknown
       Copyright(c) 1984-2020  President and Fellows of Harvard College
                              All Rights Reserved
   Current operating system: Linux-3.10.0-862.14.4.el7.x86_64(x86_64)@cn0858
                 Created on 10/15/20 at 11:58:24 by user: btmiller

            Maximum number of ATOMS:    360720, and RESidues:      120240
</pre></div><p>
At this point the program is expecting input, starting with a title; it is recommended to type <kbd>bomlev -1</kbd>
as the first command, as that will forgive typing errors and allow the program to continue.  It also recommended
to have the initial setup commands (reading RTF and PARAM files, PSF and COOR files, etc.) in a 'stream' file, so
that those actions can be done vie e.g.
<div class="term"><pre>
stream init.str
</pre></div><p>
The same applies to other complex setups, such as establishing restraints, or graphics setup.<p>
Note that the graphics uses X11, so the initial login to biowulf should use either the -X or the -Y
option of the ssh command or use NoMachine, to enable X11 tunneling for the graphics display.

<a class="navbar-safe-anchor"  Name="param"></a><div class="heading">Force Field Parameters</div><p>
Recent versions of the distributed CHARMM parameters, including the latest release, are 
available in <kbd>/usr/local/apps/charmm</kbd> as subdirectories <kbd>topparYYYY</kbd> where YYYY is the release year.
Each release contains a number of corrections and additions from the past year, esp. for the CHARMM force
fields.  The toppar2017 and later releases uses a newer format, so one should be careful about using it with older PSF files.
Files distributed with models built using CHARMM-GUI now use this newer format.

<a class="navbar-safe-anchor"  Name="bench"></a><div class="heading">Benchmarks</div><p>
The plot below is from CHARMM benchmarks run during the beta test phase of Biowulf, and shows results
for DOMDEC on Infiniband nodes (solid lines) and DOMDEC_GPU on <b>gpu</b> nodes (dotted lines), for
even numbers from 2 through 16 nodes.  The timings in ns/day are from short MD simulations, run with
a 1 fs integration time step, for 3 molecular systems of different sizes and shapes:<ul>
<li> <b>moesin</b>; 140K atoms; aqueous protein and lipid bilayer in a tetragonal unit cell, with c > a=b (tall)
<li> <b>DNPC</b>; 210K atoms; aqueous pure DNPC (24:1 chains) bilayer in a tetragonal unit cell, with a=b > c (short)
<li> <b>GroEL</b>; 468K atoms; aqueous GroEL assembly in a cubic unit cell, a=b=c</ul>
<center><img src="BW2c39b2Bench.png" width=800 height=548></center><p>
Note that the ns/day rate would be doubled with the use of a 2 fs time step, which is often done for more
exploratory sampling, but not necessarily recommended for the best accuracy and precision. Simulations
systems that cannot use DOMDEC will be somewhat slower, and will not scale well past about 64 cores.
<a class="navbar-safe-anchor"  Name="resource"></a><div class="heading">Other CHARMM resources</div><p>
<ul>
<LI><A HREF="http://www.charmm.org"><B>CHARMM.org</A> Homepage</B>
<LI><A HREF="http://charmm.chemistry.harvard.edu/"><B>Harvard University</A> CHARMM Website</B>
<LI><A HREF="http://www.charmmtutorial.org"><B>Tutorial</A> Wiki</B>
<LI><A HREF="http://mackerell.umaryland.edu/charmm_ff.shtml"><B>Force Fields</A> From Alex MacKerell</B>
<LI><A HREF="https://cgenff.paramchem.org/"><B>ParamChem</A> Using CGenFF to add new molecules </B>
<LI><A HREF="http://www.charmm-gui.org/"><B>CHARMM-GUI;</A> model building, esp. membrane systems</B>
<LI><A HREF="http://www.charmming.org/charmming/"><B>CHARMMing;</A> learning site, emphasis on QM/MM, redox</B>
</ul>

<hr>
<center><b>The CHARMM program is provided and maintained on Biowulf with the assistance of the NHLBI
<a href="http://www.lobos.nih.gov/lcb/index.shtml"> Laboratory of Computational Biology</a></b></center>
<hr>
<ADDRESS>
CHARMM on Biowulf / <A HREF="mailto:Rick_Venable@nih.gov"> Rick_Venable@nih.gov </A> and <a href="mailto:staff@hpc.nih.gov">staff@hpc.nih.gov</a>
</ADDRESS>

<!-- End content area - do not edit below this line -->
<script type="text/javascript" language="JavaScript" src='/js/footer.js'></script>
