<script type="text/javascript" language="JavaScript" src='/js/header.js'></script>
<!-- Start content - do not edit above this line  -->
<script type='text/javascript' language='JavaScript'>document.querySelector('title').textContent = 'Snakemake on Biowulf';</script>
<div class="title">Snakemake on Biowulf</div>

<table width=25% align=right>
  <tr>
    <td>
      <div class="toc">
        <div class="tocHeading">Quick Links</div>
        <div class="tocItem"><A href="#doc">Documentation</a></div>
        <div class="tocItem"><a href="#notes">Notes</a></div>
        <div class="tocItem"><a href="#profile">Biowulf profile</a></div>
        <div class="tocItem"><a href="#int">Interactive job </a></div>
        <div class="tocItem"><a href="#sbatch">Batch job </a></div>
         <div class="tocItem"><a href="#group">Job groups</a></div>
      </div>
</table>

<p>Snakemake aims to reduce the complexity of creating workflows by providing a
fast and comfortable execution environment, together with a clean and modern
domain specific specification language (DSL) in python style.</p>

<h3>References:</h3>
<ul>
    <li>Johannes KÃ¶ster and Sven Rahmann. <em>Snakemake--a scalable bioinformatics 
        workflow engine.</em> Bioinformatics 2012, 28:2520-2522.
    <a href="http://www.ncbi.nlm.nih.gov/pubmed/22908215">Pubmed</a>&nbsp;|&nbsp;
    PMC &nbsp;|&nbsp;
    <a href="http://bioinformatics.oxfordjournals.org/content/28/19/2520.long">
        Journal</a></li>
</ul>


<a Name="doc"></a><div class="heading">Documentation</div>
<ul>
    <li><a href="https://bitbucket.org/snakemake/snakemake/wiki/Documentation">
        Documentation</a></li>
    <li><a href="https://bitbucket.org/snakemake/snakemake/wiki/Tutorial">
        Tutorial</a></li>
    <li><a href="https://speakerdeck.com/johanneskoester/workflow-management-with-snakemake">
        Workflow management with snakemake</a></li>
    <li><a href="http://marcelm.net/talks/2015/snakemake/#/">Snakemake makes ... snakes?</a>
    </li>
    <li><a href="http://metagenomic-methods-for-microbial-ecologists.readthedocs.org/en/latest/day-1/">
        Metagenomics with snakemake</a></li>
    <li><a href="https://github.com/leipzig/SandwichesWithSnakemake">Making sandwiches</a>
    </li>
</ul>

<a Name="notes"></a><div class="heading">Important Notes</div>
<ul>
    <li><b style="background-color: #FFFF99;">Please use --max-jobs-per-second and --max-status-checks-per-second
        (see below) in your command line or profile to limit the number of calls to slurm.</b></li>
    <li>Snakemake is present in the Python 3 environments and as a standalone snakemake
    module (see <a href="/apps/modules.html">the modules page</a> for more information)</li>
    <li>Snakemake can run jobs on a single machine making use of multiple CPUs and/or submit 
    jobs as Slurm batch jobs. <b style="background-color: #FFFF99;">Even if you are submitting jobs as batch jobs, please do
        not run Snakemake on the login node.</b></li>
</ul>
<P>

<a Name="profile"></a><div class="heading">Biowulf snakemake profile</div>
<p>You can use our <a href="https://github.com/NIH-HPC/snakemake_profile">snakemake profile</a> which takes
generic resource keys/values and automatically determines partition assignment for jobs. It also uses a low
impact method of querying for jobs.</p>

<div class="heading">Example pipeline</div>

<p>For a general introduction on how to use snakemake please read through the official
<a href="https://bitbucket.org/johanneskoester/snakemake/wiki/Documentation">Documentation
</a> and <a href="https://bitbucket.org/johanneskoester/snakemake/wiki/Tutorial">
Tutorial</a> or any of the other materials mentioned above.</p>

<p>In addition, you can have a look at a set of 
<a href="https://github.com/NIH-HPC/snakemake-class"</a>exercises</a> on our
GitHub site.</p>

<a Name="int"></a><div class="heading">Interactive job</div>
<div class="nudgeblock"><a href="/docs/userguide.html#int">Interactive jobs</a> should be used for debugging, graphics, or applications that cannot be run as batch jobs.</div>

<p>Allocate an <a href="/docs/userguide.html#int">interactive session</a>
and use as described below. If all tasks are submitted as cluster jobs this
session may only require 2 CPUs. If some or all of the rules are run locally (i.e.
within the interactive session) please adjust the resource requirements
accordingly. In the example above there are at least 3 rules that are going to be run
locally so we will request 8 CPUs.</p>

<pre class="term">
[user@biowulf]$ <b>sinteractive --mem=6g --cpus-per-task=8</b>
salloc.exe: Pending job allocation 46116226
salloc.exe: job 46116226 queued and waiting for resources
salloc.exe: job 46116226 has been allocated resources
salloc.exe: Granted job allocation 46116226
salloc.exe: Waiting for resource configuration
salloc.exe: Nodes cn3144 are ready for job

[user@cn3144 ~]$ <b>module load snakemake</b>
[user@cn3144 ~]$ <b># for the local rules</b>
[user@cn3144 ~]$ <b>module load samtools/1.3.1</b>
[user@cn3144 ~]$ <b>cd /path/to/snakefile</b>
[user@cn3144 ~]$ <b>ls -lh</b>
-rw-rw-r-- 1 user group  606 Jan 30 12:56 cluster.json
-rw-rw-r-- 1 user group 4.0K May 12  2015 config.yaml
-rw-rw-r-- 1 user group 4.8K Jan 30 15:06 Snakefile
</pre>

<p>Run the pipeline locally (i.e. each task would be run as part
of this interactive session). Note that snakemake
will by default assume that your pipeline is in a file called 'Snakefile'. If
that is not the case, provide the filename with <code>'-s SNAKEFILENAME'</code>.
<em>This would take a long time</em>: </p>
<pre class="term">
[user@cn3144 ~]$ <b>snakemake -pr --keep-going -j $SLURM_CPUS_PER_TASK all</b>
Provided cores: 8
Job counts:
        count   jobs
        14      align
        1       all
        14      clean_fastq
        14      fastqc
        6       find_broad_peaks
        8       find_narrow_peaks
        14      flagstat_bam
        14      index_bam
        85
[...snip...]
</pre>

<p>To submit subjobs that are not marked as <code>localrules</code> to the cluster it
is necessary to provide an sbatch template string using variables from the 
Snakefile or the cluster configuration file. <span style="background-color: #FFFF99;">Please use <code>--max-jobs-per-second</code>
and <code>--max-status-checks-per-second</code> in your command line or profile to limit
the number of calls to slurm</span>:</p>

<pre class="term">
[user@cn3144 ~]$ <b>sbcmd="sbatch --cpus-per-task={threads} --mem={cluster.mem}"</b>
[user@cn3144 ~]$ <b>sbcmd+=" --time={cluster.time} --partition={cluster.partition}"</b>
[user@cn3144 ~]$ <b>sbcmd+=" --out={cluster.out} {cluster.extra}"</b>
[user@cn3144 ~]$ <b>snakemake -pr --keep-going --local-cores $SLURM_CPUS_PER_TASK \
             --jobs 10 --cluster-config cluster.json --cluster "$sbcmd" \
             --max-jobs-per-second 1 --max-status-checks-per-second 0.01 \
             --latency-wait 120 all</b>
Provided cluster nodes: 10
Job counts:
        count   jobs
        14      align
        1       all
        14      clean_fastq
        14      fastqc
        6       find_broad_peaks
        8       find_narrow_peaks
        14      flagstat_bam
        14      index_bam
        85
[...snip...]
[user@cn3144 ~]$ <b>exit</b>
salloc.exe: Relinquishing job allocation 46116226
[user@biowulf ~]$
</pre>

<p>Note that <code>--latency-wait 120</code> is required for pipelines that
submit cluster jobs as output files generated on other nodes may not
become visible to the parental snakemake job until after some delay.</p>


<P>
<a Name="sbatch"></a><div class="heading">Batch job</div>
<div class="nudgeblock">Most jobs should be run as <A href="/docs/userguide.html#submit">batch jobs</a>.</div>

<p>In this usage mode the main snakemake job is itself submitted as a batch job.
It is still possible to either run all the rules locally as part of the single
job or have the main job submit each (non-local) rule as another batch job as
described above. The example below uses the latter pattern.</p>

<p>Create a batch input file (e.g. snakemake.sh) similar to the following:</p>

<pre class="term">
#! /bin/bash
# this file is snakemake.sh
module load snakemake samtools || exit 1

sbcmd="sbatch --cpus-per-task={threads} --mem={cluster.mem}"
sbcmd+=" --time={cluster.time} --partition={cluster.partition}"
sbcmd+=" --out={cluster.out} {cluster.extra}"

snakemake -pr --keep-going --local-cores $SLURM_CPUS_PER_TASK \
    --jobs 10 --cluster-config cluster.json --cluster "$sbcmd" \
    --latency-wait 120 all
</pre>
<p>Submit this job using the Slurm <a href="/docs/userguide.html">sbatch</a> command.</p>

<pre class="term">sbatch --cpus-per-task=2 --mem=8g snakemake.sh</pre>


<a Name="group"></a><div class="heading">Job groups</div>

<p>For pipelines with many short running jobs we highly recomment you explore
job groups to combine multiple tasks into single batch jobs. Here is a trivial
example pipeline where for each of 5 input "samples" three jobs are executed before
results are summarized. Resources are specified in the Snakefile directly:</p>

<pre class="term">
ids = [1, 2, 3, 4, 5]

localrules: all
rule all:
    input: expand("stage3/{id}.3", id=ids)
    output: "output/summary"
    shell: "cat {input} &gt; {output}"

rule stage1:
    input: "input/{id}"
    output: "stage1/{id}.1"
    threads: 2
    resources: mem_mb=1024, runtime=5
    shell: "cp {input} {output}"

rule stage2:
    input: "stage1/{id}.1"
    output: "stage2/{id}.2"
    threads: 2
    resources: mem_mb=1024, runtime=5
    shell: "cp {input} {output}"

rule stage3:
    input: "stage2/{id}.2"
    output: "stage3/{id}.3"
    threads: 2
    resources: mem_mb=2048, runtime=5
    shell: "cp {input} {output}"
</pre>


<div style="width: 40%; margin: 0 auto">
  <img src="/images/snakemake_dag.png" width="100%"/>
</div>

<p>A typical run of this pipeline might use the command below. Note that for this
trivial example we are running from an sinteractive session. However, normally this
would be submitted as a batch job.</p>
<pre class="term">
[user@cn3144]$ <b>snakemake -j5 --latency-wait=180 \
    --cluster="sbatch -c {threads} --mem={resources.mem_mb} --time={resources.runtime} -p quick"</b>
</pre>

<p>This pipeline ran 15 slurm jobs each with a time limit of 5 minutes, 2 CPUs,
and either 1024MB or 2048MB of memory. Obviously each job had a very short
actual runtime. Given that it takes a fixed overhead to schedule jobs this is
not optimal for cluster efficiency. In addition the queue wait time for each
job slows down the pipeline.</p>

<p>We can improve on this by grouping each of the steps for a sample into a
single job. This is done by assigning rules to groups either with the
<code>group:</code> keyword in the snakefile or from the command line like
this:</p>

<pre class="term">
[user@cn3144]$ <b>snakemake -j5 --latency-wait=180 \
    --groups stage1=grp1 stage2=grp1 stage3=grp1 \
    --cluster="sbatch -c {threads} --mem={resources.mem_mb} --time={resources.runtime} -p quick"</b>
</pre>

<p>In this example, the number of slurm jobs was reduced from 15 to 5. Each job
still used 2 CPUs but was allocated 2048MB of memory (the max). Unfortunately
snakemake is not yet smart enough to sum the time instead of taking the max.
This may be fixed in the future. Note that it may not make sense to group jobs
with very different resource requirements. This is an illustration for what
snakemake did:</p>

<div style="width: 40%; margin: 0 auto">
  <img src="/images/snakemake_dag_group.png" width="100%"/>
</div>

<p>Next we can combine multiple "samples" into a single job. For example, to
combine 3 samples per job we could run</p>

<pre class="term">
[user@cn3144]$ <b>snakemake -j5 --latency-wait=180 \
    --groups stage1=grp1 stage2=grp1 stage3=grp1 \
    --group-components grp1=3 \
    --cluster="sbatch -c {threads} --mem={resources.mem_mb} --time={resources.runtime} -p quick"</b>
</pre>

<p>This results in 2 jobs - one with 6 CPUs, 6GB, and 15 minute time limit for
3 of the samples and one with 4 CPUs, 4GB, and 10 minutes time limit. Notice how
again snakemake is not yet sophisticated enough to distinguish between the
requirements for CPUs/memory vs runtime. That may mean that some manual
intervention may be needed to make this work properly. Here is the visualization
of this run:</p>

<div style="width: 40%; margin: 0 auto">
  <img src="/images/snakemake_dag_group_components.png" width="100%"/>
</div>



<!-- the object store is no more -->
<!-- <a Name="objstore"></a><div class="heading">Using remote files on our object store</div>

<p>Snakemake can make use of remote files stored on a number of different file
storage services (AWS S3, Google cloud storage, sftp, http(s), ...). The S3 storage
provider can be adapted to use data stored on the HPC object store. If you haven't done
so yet, please set up a <code>~/.boto</code> or <code>~/.aws/credentials</code> file as describe in our 
<a href="https://hpc.nih.gov/storage/object.html#object_howto">object storage user guide</a>.
</p>

<p>By default snakemake will fetch the remote file into the current working directory
(the directory where snakemake was started) when a rule needs it and remove it when
no other rules depend on it. Since this is going to be a /data directory, it would be more
efficient to specify <code>stay_on_remote=True</code> and fetch the file to lscratch
or pipe it into downstream tools or use tools that can access the object store themselves.
The latter options come at the price of more complexity.</p>

<p>In the following example replace VAULT with your vault name.</p>
<pre class="term">
# disable warnings about the object store's certificate
import warnings
warnings.filterwarnings("ignore", "Unverified HTTPS request is being made")

# pick one of the accessors os{1,2}naccess{1,2,3}
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(endpoint_url="https://os1naccess1", verify=False)

rule remote_in_cwd:
    """
    snakemake will automatically fetch the object from the object store
    as needed and remove it when done. The local copy will be in the current
    working directory
    """
    input: S3.remote("VAULT/some_project/aln/sample1.bam")
    #                       ^--------------------------- object name starts here
    output: "sample1.flagstat"
    shell:
        """
        module load samtools
        samtools flagstat {input} &gt; {output}
        """

rule stream_from_remote:
    """
    Don't have snakemake store a local copy of the object. Use a tool
    that can directly access data on the object store. Our version of
    samtools can do that, for example.
    """
    input: S3.remote("VAULT/some_project/aln/sample2.bam", stay_on_remote=True)
    output: "sample2.flagstat"
    shell:
        """
        module load samtools
        objname=$(echo {input:q} | sed -e 's|^s3://||')
        samtools flagstat s3+http://obj@$objname &gt; {output}
        """

rule copy_to_lscratch:
    """
    Don't have snakemake store a local copy of the object. Copy it to
    lscratch instead. Again, some name mangling is required
    """
    input: S3.remote("VAULT/some_project/aln/sample3.bam", stay_on_remote=True)
    output: "sample3.flagstat"
    shell:
        """
        module load samtools
        bam=$(echo {input:q} | sed -e 's|^s3://VAULT/||')
        obj get -D /lscratch/$SLURM_JOB_ID $bam
        samtools flagstat  /lscratch/$SLURM_JOB_ID/$bam &gt; {output}
        """
</pre>
-->

<!-- End content area - do not edit below this line -->
<script type="text/javascript" language="JavaScript" src='/js/footer.js'></script>
