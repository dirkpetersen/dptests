<script type="text/javascript" language="JavaScript" src='/js/header.js'></script>
<!-- Start content - do not edit above this line  -->
<script type='text/javascript' language='JavaScript'>document.querySelector('title').textContent = 'pychopper on Biowulf';</script>
<div class="title">pychopper on Biowulf</div>

<table width=25% align=right>
    <tr>
        <td>
            <div class="toc">
                <div class="tocHeading">Quick Links</div>
                <div class="tocItem"><A href="#doc">Documentation</a></div>
                <div class="tocItem"><a href="#notes">Notes</a></div>
                <div class="tocItem"><a href="#int">Interactive job </a></div>
                <div class="tocItem"><a href="#sbatch">Batch job </a></div>
                <div class="tocItem"><a href="#swarm">Swarm of jobs </a></div>
            </div>
        </td>
    </tr>
</table>

<p>Pychopper is used to identify, orient and trim full-length Nanopore
cDNA reads. The tool is also able to rescue fused reads.</p>

<!--
<h3>References:</h3>
<ul>
<li>Paper</li>
</ul>
-->

<a Name="doc"></a><div class="heading">Documentation</div>
<ul>
<li>pychopper on <a href="https://github.com/nanoporetech/pychopper">GitHub</a></li>
</ul>

<div class="heading">Important Notes</div>
<ul>
    <li>Module Name: pychopper (see <a href="/apps/modules.html">the modules page</a> for more information)</li>
    <li>pychopper can use multiple CPUs. Please match your allocation with the number
        of threads used. pychopper does not scale efficiently to more than 8 CPUs</li>
    <li>pychopper &le; 2.4.0 cannot natively read gzip input. &ge;2.7.1 can read gzip files but
    it is inefficient. It remains more efficient to uncompress input to lscratch for processing.</li>
    <li>pychopper often reads the fastq file twice. Therefore it's best to move/unpack fastq files
        into lscratch. See example below.</li>
    <li> <code>-m edlib</code> is associated with stalled runs and should probably be avoided.</li>
    <li>Example files in <code>$PYCHOPPER_TEST_DATA</code></li>
    <li>the command <code>cdna_classifier.py</code> was renamed to <code>pychopper</code> between 2.4.0 and 2.7.1</li>
</ul>
<P>

<a Name="int"></a><div class="heading">Interactive job</div>
<div class="nudgeblock"><a href="/docs/userguide.html#int">Interactive jobs</a> should be used for debugging, graphics, or applications that cannot be run as batch jobs.</div>
<p>Allocate an <a href="/docs/userguide.html#int">interactive session</a> and run the program. Sample session:</p>
<pre class="term">
[user@biowulf]$ <b>sinteractive --mem=10g --cpus-per-task=6 --gres=lscratch:10</b>
salloc.exe: Pending job allocation 46116226
salloc.exe: job 46116226 queued and waiting for resources
salloc.exe: job 46116226 has been allocated resources
salloc.exe: Granted job allocation 46116226
salloc.exe: Waiting for resource configuration
salloc.exe: Nodes cn3144 are ready for job

[user@cn3144]$ <b>cd /lscratch/$SLURM_JOB_ID</b>
[user@cn3144]$ <b>module load pychopper</b>
[user@cn3144]$ <b>zcat $PYCHOPPER_TEST_DATA/SIRV_E0_pcs109_25k.fq.gz &gt; input.fastq</b>
[user@cn3144]$ ## pychopper was called cdna_classifier.py in versions &le; 2.4.0
[user@cn3144]$ <b>pychopper -r report.pdf -u unclassified.fastq -t $SLURM_CPUS_PER_TASK \
                    -w rescued.fastq input.fastq - | gzip -c &gt; /data/$USER/temp/full_length.fastq.gz</b>
Using kit: PCS109
Configurations to consider: "+:SSP,-VNP|-:VNP,-SSP"
Total fastq records in input file: 25000
Tuning the cutoff parameter (q) on 9465 sampled reads (40.0%) passing quality filters (Q &ge; 7.0).
Optimizing over 30 cutoff values.
100%|████████████████████████████████████████████████████████| 30/30
Best cutoff (q) value is 0.3448 with 88% of the reads classified.
Processing the whole dataset using a batch size of 4166:
 94%|██████████████████████████████████████████████████      | 23614/25000
Finished processing file: input.fastq
Input reads failing mean quality filter (Q &lt; 7.0): 1386 (5.54%)
Output fragments failing length filter (length &lt; 50): 0
-----------------------------------
Reads with two primers: 86.93%
Rescued reads:          3.16%
Unusable reads:         9.91%
-----------------------------------

</pre>
<p>Move the rescuted and unclassified reads and the reports if you need them before
ending the session.</p>
<pre class="term">
[user@cn3144]$ <b>gzip -c rescued.fastq > /data/$USER/temp/rescued.fastq.gz</b>
[user@cn3144]$ <b>gzip -c unclassified.fastq > /data/$USER/temp/unclassified.fastq.gz</b>
[user@cn3144]$ <b>mv report.pdf /data/$USER/temp</b>
[user@cn3144]$ <b>exit</b>
salloc.exe: Relinquishing job allocation 46116226
[user@biowulf]$
</pre>

<P>
<a Name="sbatch"></a><div class="heading">Batch job</div>
<div class="nudgeblock">Most jobs should be run as <A href="/docs/userguide.html#submit">batch jobs</a>.</div>
<p>Create a batch input file (e.g. pychopper.sh), which uses the input file 'pychopper.in'. For example:</p>

<pre class="term">
#!/bin/bash
module load pychopper/2.7.1 || exit 1
cd /lscratch/$SLURM_JOB_ID
zcat $PYCHOPPER_TEST_DATA/SIRV_E0_pcs109_25k.fq.gz &gt; input.fastq
## use cdna_classifier.py instead of pychopper for versions &le; 2.4.0
pychopper -r report.pdf -u unclassified.fastq -t $SLURM_CPUS_PER_TASK \
    -w rescued.fastq input.fastq - | gzip -c &gt; /data/$USER/temp/full_length.fastq.gz
gzip -c rescued.fastq &gt; /data/$USER/temp/rescued.fastq.gz
gzip -c unclassified.fastq &gt; /data/$USER/temp/unclassified.fastq.gz
mv report.pdf /data/$USER/temp
</pre>
<p>Submit this job using the Slurm <a href="/docs/userguide.html">sbatch</a> command.</p>

<pre class="term">sbatch --cpus-per-task=6 --mem=10g pychopper.sh</pre>


<a Name="swarm"></a><div class="heading">Swarm of Jobs </div>
<div class="nudgeblock">A <a href="/apps/swarm.html">swarm of jobs</a> is an easy way to submit a set of independent commands requiring identical resources.</div>

<p>Create a swarmfile (e.g. pychopper.swarm). For example:</p>

<pre class="term">
zcat input1.fastq.gz &gt; /lscratch/$SLURM_JOB_ID/input1.fastq && \
  pychopper -t $SLURM_CPUS_PER_TASK /lscratch/$SLURM_JOB_ID/input1.fastq - \
  | gzip -c &gt; /data/$USER/temp/full_length1.fastq.gz
zcat input2.fastq.gz &gt; /lscratch/$SLURM_JOB_ID/input1.fastq && \
  pychopper -t $SLURM_CPUS_PER_TASK /lscratch/$SLURM_JOB_ID/input2.fastq - \
  | gzip -c &gt; /data/$USER/temp/full_length2.fastq.gz
</pre>

<p>Submit this job using the <a href='/apps/swarm.html'>swarm</a> command.</p>

<pre class="term">swarm -f pychopper.swarm -g 10 -t 6 --module pychopper/2.0.3</pre>
where
<table border=0>
  <tr><td width=20%>-g # <td>Number of Gigabytes of memory required for each process (1 line in the swarm command file)
  <tr><td>-t # <td>Number of threads/CPUs required for each process (1 line in the swarm command file).
  <tr><td>--module pychopper <td>Loads the pychopper module for each subjob in the swarm 
</table>




<!-- End content area - do not edit below this line -->
<script type="text/javascript" language="JavaScript" src='/js/footer.js'></script>
