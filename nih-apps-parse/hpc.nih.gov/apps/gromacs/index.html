<script type="text/javascript" language="JavaScript" src='/js/header.js'></script>
<!-- Start content - do not edit above this line  -->
<script type='text/javascript' language='JavaScript'>document.querySelector('title').textContent = ' Gromacs Benchmarks on Biowulf';</script>
<div class="title">
Gromacs Benchmarks on Biowulf</div>
<P>
Please read <a href="https://hpc.nih.gov/policies/multinode.html">Making Effective Use of Biowulf's multinode partition</a> before running multinode Gromacs jobs.
<P>
Gromacs overview of parallelization and acceleration schemes: <a href="https://manual.gromacs.org/documentation/current/user-guide/mdrun-performance.html"> Getting good performance from mdrun</a>
<P>
For information about Gromacs performance, see the following papers:
<ol>
<li> <a href="http://arxiv.org/abs/1506.00716">PÃ¡ll, et al. (2015) Proc. of EASC 2015 LNCS, 8759 3-27</a>. (<a href="https://doi.org/10.1007/978-3-319-15976-8_1">DOI</a>)
<li> <a href="http://onlinelibrary.wiley.com/doi/10.1002/jcc.24030/full">Kutzner, et al. (2015) J. Comput. Chem, 36 1990-2008</a> (<a href="http://doi.org/10.1002/jcc.24030">DOI</a>)
<li> <A href="http://www.sciencedirect.com/science/article/pii/S2352711015000059">Abraham, et al. (2015) SoftwareX 1-2 19-25</a> (<a href="http://dx.doi.org/10.1016/j.softx.2015.06.001">DOI</a>)
</ol>
<P>
[<A href="index.2018.3.html">Gromacs 2018.3 benchmarks</a>]
<P>

<div class="heading">Gromacs 2022.4: ADH Cubic Benchmark on CPU </div>
<P>
<b>February 2023</b><br>
Gromacs 2022.4 was built with gcc 9.2.0, CUDA 11.0, and OpenMPI 4.1.3.
The ADH benchmark suite is <a href="https://ftp.gromacs.org/pub/benchmarks/">available on the Gromacs website</a>.
<P>
All runs were performed on the Biowulf  Phase2 nodes,
which are 28 x 2.3 GHz (Intel E5-2695v3), hyperthreading enabled, connected via 56 Gb/s FDR Infiniband (1.11:1).
<P>
<center>
<img src="Picture1fincpcu.png" width=850>
<table border=1 class="horztable">
<tr><Td width=13% class="softBottom">ntomp = 1  <td width=13% class="softBottom">blue line <td class="softBottom"> 1 thread per MPI task, 1 MPI task per physical core. No hyperthreading
<tr><td class="softBottom">ntomp = 2-noht <td class="softBottom">orange line <td class="softBottom">2 threads per MPI task, no hyperthreading. This uses twice as many cores as the options above, but with threads per core = 1.
<tr><td class="softBottom">ntomp = 2-ht <td class="softBottom">grey line <td class="softBottom">2 threads per MPI task, hyperthreading enabled. This effectively uses the same number of cores as ntomp=1, but uses both hyperthreaded
CPUs on each core.
</table>
<center>
<table border=1 cellpadding=10>
<tr><td><b># MPI tasks<br> <td><b>ntomp = 1<br>ns/day<td><b>ntomp = 2-noht<br>ns/day<td><b>ntomp = 2-ht<br>ns/day
<tr><Td>1 <td align=right>0.745<td align=right>0.866<td align=right>0.867
<tr><Td>2 <td align=right>1.448<td align=right>1.669<td align=right>1.672
<tr><Td>4 <td align=right>2.828<td align=right>3.244<td align=right>3.244
<tr><Td>8 <td align=right>5.372<td align=right>6.132<td align=right>6.121
<tr><Td>16<td align=right>9.861<td align=right>10.63<td align=right>10.909
<tr><Td>32 <td align=right>16.28<td align=right>17.803<td align=right>17.666
<tr><td>64 <td align=right>28.164<td align=right>31.117<td align=right>30.872
<tr><td>128 <td align=right>43.785<td align=right>44.728<td align=right>44.651
<tr><td>256 <td align=right>67.001<td align=right>61.482<td align=right>61.008
<tr><td>512 <td align=right>87.604<td align=right>78.883<td align=right>78.601
</table>
</center>
<P><hr>
<td>For this benchmark, it is recommended that for MPI tasks < 128 to use ntomp = 1 hyperthreading disabled. 
 <p>Larger numbers of MPI tasks should use ntomp = 2, all MPI tasks run most effectively assigning ntasks-per-core = 1. 
 </pre>
<P>
</pre>
</table>
<P>
<div class="heading">Gromacs 2022.4: ADH Cubic Benchmark on GPU</div>
<P>
<b>[Jan 2023]</b>  The ADH benchmark suite is <a href="https://ftp.gromacs.org/pub/benchmarks/">available on the Gromacs website</a>. The tests below used
  the ADH_cubic benchmark.. 
<p>

For running gromacs on GPU: <a href="https://manual.gromacs.org/documentation/current/user-guide/mdrun-performance.html#running-mdrun-with-gpus">Running mdrun with GPUs</a>. 
<P>
k80's and v100's are Biowulf phase3 nodes Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz.
<p>
v100x's are Biowulf phase5 nodes Intel Gold 6140 @ 2.30GHz.
<p>
p100's are Biowulf phase4 nodes Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz.
<p>
a100s's are Biowulf phase6 nodes AMD EPYC 7543P 32-Core Processor.
<p>
<table border=0>
<tr><td><img src="gpu2.png" width=600>
<td>For running GPU jobs, please use the freen command to determine which GPU paritions are available. For running these jobs, an appopriate submission command would be:
<pre class="term">
sbatch --partition=gpu \
--gres=gpu:xxx:1 --ntasks=2 \
--ntasks-per-core=1 --exclusive jobscript
</pre>
<P>
For this benchmark, there is a significant advantage to running Gromacs a100 nodes over k80 nodes. There is typically no advantage to running on more than a single GPU device. 
<p>
Single-GPU jobs were run on a single GPU with ntasks=2, n-tasks-per-core=1, and ntomp=2.
<P>
These results may be different for larger molecular systems, so please run your own benchmarks to verify that
your jobs benefit from the GPUs (and send us any interesting results!)
<P>
</table>
<!-- End content area - do not edit below this line -->
<script type="text/javascript" language="JavaScript" src='/js/footer.js'></script>
